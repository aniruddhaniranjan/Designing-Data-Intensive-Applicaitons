If leader exists, client sends requests to one leader replica and database system copies the write to other replicas. The leader determines the order in which replicas see the write
In leaderless, client sends the write requests to several replicas or via a coordinator node which sends to several replicas. The coordinator may not enforce a particular ordering of writes.

Writing to the database when a node is down - In a leader-based config we wait for failover. In leaderless, there is no failover. If an acceptable number of replicas successfully process the write request, then the client assumes the operation is successful and ignores the leader replica which is down. When the unavailable node comes back, it will have stale data. To solve this - client sends read requests to several replicas, and might get back different results. Version numbers are used to determine which value is up-to-date and which value is stale

Read repair and anti-entropy - The purpose of this is to ensure that all the replicas eventually get all the write requests/all the data.
a. Read repair - When a client makes a read request to several replicas, it can detect the stale value using the version numbers. Then it writes back the up-to-date value to the replica which had the stale value. This works well for values that are frequently read.
b. Anti-entropy process - Some datastores have a background process which constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike replication log, there is no particular order in which writes are copied and there may be a significant delay before which the writes are updated.
Without an anti-entropy process, values that are frequently read might not be updated for a long time. Hence they have reduced durability or higher susceptability to data loss.

Quorums for reading and writing - Similar to generic case. If there are n nodes, we need to write to w nodes and read from r nodes where w + r > n. These are called quorum reads and writes. We can expect to get up-to-date values because at least one of the r nodes will have an up-to-date value. (r and w are the minimum number of votes required for the reads and writes to be successful). Typically, make n an odd  number and make r and w = (n+1)/2. But some cases are different - for few writes and large reads make w = n and r = 1 (but this makes write fail even if one node is down)

Reads and writes are always sent to all n replicas in parallel - w and r determine how many nodes we wait for (how many of the nodes need to report success before we consider the read or write to be successful). If fewer than w or r nodes return success, then reads or writes return an error. Reasons for node to be unavailable - node is down (crashed/powered down), error executing the operation (can't write because disk is full), network interruption between client and node, etc. We only care about success/failure and not about the cause of failure.

Limitations of Quorum Consistency - Usually w and r values are set to majority (more than n/2) so that the nodes to which we write to and read from must overlap at minimum of one node. But, it's not necessary to have them as majority. Here, reads and writes are still sent to n nodes, but a smaller number of nodes need to return successfully in order for the operation to be considered successfull. There is a greater chance of reading stale data in case w + r < n. This is because it's likely that the read operations 'r' didn't include any nodes which had the latest date from the 'w' succesfull writes. Advantage - lower latency and higher availability, even if many replicas become unavailable we can continue processing reads and writes (until the number of available replicas falls below w & r)

Edge cases where stale data can be read even when 'w + r > n':
a. If sloppy quorum is used, then the reads 'r' can end up on nodes different from the 'w' writes, no guaranteed overlap
b. If two writes occur concurrently, it is not clear which one happened first. Writes can always be lost - e.x. due to clock skew if merge is resolved based on timestamp.
c. If read and write happen simultaneously, it's not certain if the read returns the old or new value - read/write happen on diff nodes
d. If a write succeeds in fewer than w replicas, the operation is considered to have failed but the write is not rolled back on the replicas where it succeeded. The reads later on may or may not return the value from this write.
e. If a node carrying a new value fails, and the data is restored from a node which has the old value, then the write quorum may no longer be satisfied
f. Timing issues - Linearizability and quorums

Dynamo-style databases are optimized for use cases that can tolerate eventual consistency. Parameters w and r allow to change probability of values read being stale - but it can't be taken as guarantees. Guarantees such as 'reading your writes', 'monotonic reads', or 'consistent prefix reads' can't be made. The solutions then are similar to transactions or consensus. 

Monitoring Staleness - It is important to monitor how up-to-date the results returned by DBs are, even if staleness can be tolerated by the system. If it falls behind significantly, it should alert so that cause can be investigated (overloaded node, nw problem)
For leader-based replication, DB typically exposes metrics for replication lag (can be tied to a monitoring system). This is because writes are applied to leader and then to followers from the replication log. Each node has a position in the replication log, and the difference in node's position and leader's current position can indicate the amount of replication lag
In leaderless systems there is no ordering of writes - tougher to measure lag. If system uses only read-repair and no anti-entropy, then in the case of infrequent reads, the values read might be very old. It is a good idea to include staleness measurements as part of DB metrics - although eventual consistency is a vague guarantee, quantifying it is good for operability

Sloppy Quorums and Hinted Handoff
DBs with properly configured quorums can tolerate failed individual nodes w/o failover. Even if individual nodes are slow, it can be tolerated. So, leaderless replication is a good choice for high availability, low latency systems, if some staleness is tolerated.
When a certain set of nodes are unreachable to a client, they might still be reachable to a different client
In a system with much larger than n nodes, a client maybe able to reach some DB nodes, but not the group of nodes required for quorum, for a successful write of a particular value - a. Return errors for all requests for which we can't reach a quorum of w or r nodes OR b. Accept writes anyway, and write them to some nodes which are reachable but where the value usually doesn't reside.
Option 'B' is called "Sloppy Quorum" - Success still needs w or r responses, but it could also come from nodes which are not "Home" nodes for a value. Once the nw issue is fixed, the value is sent to the appropriate "Home" node, from the nodes which temp accepted these writes. This is called "Hinted Handoff". Sloppy Quorum is good for increasing high availability. (Tolerate staleness). It is not a strict quorum, but only a guarantee of durability and staleness exists till Hinted Handoff is complete.

Multi-DC Operation - Leaderless replication is also suitable for multi DC operation since it tolerates conflicting concurrent writes, network interruptions, and latency spikes. In Cassandra and Voldemort, the n nodes includes nodes in all DCs. Writes are sent to all DCs but client wits for quorum of nodes only from local DC so that it's unaffected by inter-DC link delays. Writes to other DCs have higher latency and are done asynchronously. Riak does the communication only in local DC and cross DC writes are done asynchronously

Detecting Concurrent Writes
Dynamo style DBs allow multiple clients to write concurrently to the same key, in which case there will be conflicts even if strict quorum is used (similar to multi-leader but conflicts can occur due to read repair and hinted handoff as well).
Problem is write requests arrive in diff orders at diff nodes due to network delays, partial failures. If each node simply overwrote older values then the nodes would become permanently inconsistent. But nodes should converge - DBs don't handle this well but application developer should use some methods to handle it

Last Write Wins (discarding concurrent writes) - Declare that each replica overwrite and discard the "older" value with "recent" value. As long as there is an unambigious way to determine "recent" value, and every write is replicated to each replica, system converges. Sometimes writes are concurrent and we don't know which value is "recent". 
If a timestamp is used to determine order and writes with earlier timestamps discarded, the approach is LWW. It achieves eventual consistency but at the cost of durability - some of successful concurrent writes will be discarded. LWW may drop writes that are not event consistent. If caching is used/data loss can be tolerated LWW is a possible solution. Only way to ensure LWW fully works is to make a key/value immutable once a write is performed.

Happens-before relationship and concurrency - An operation A happens before B if B knows about A, B depends on A or B builds on top of A in some way. This is key to defining concurrency - two operations are concurrent if neither happens before the other. Either A happens before B, B happens before A or they are concurrent. If one operation happens before the other then value is overwritten and if they are concurrent then conflicts need to be resolved.

Capturing the Happens-Before relationship:
Causal dependencies between concurrent writes can be shown as below:
https://github.com/aniruddhaniranjan/Designing-Data-Intensive-Applications/blob/master/Causal%20Dependencies%201.png
Graph representation of causal dependencies can be shown as below:
https://github.com/aniruddhaniranjan/Designing-Data-Intensive-Applications/blob/master/Causal%20Dependencies%202.png

