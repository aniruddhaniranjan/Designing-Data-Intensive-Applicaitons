Things that can go wrong in data systems:
 DB software or hardware may fail (even in middle of an operation)
 Application may crash anytim (halfway thru a series of operations)
 Nw interruptions can cut off appl from DB or DB nodes from each other
 Several clients write to DB overwriting each others changes
 Client reads data that doesn't make sense since it's only partially written
 Race conditions b/w clients can cause unexpected bugs
Reliable systems have to deal with these to prevent catastrophe. Implementing fault tolerance mechanisms is tough - think about everything that can go wrong, test everything to ensure it solves the problems

A Transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, these reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback). If it fails, app can safely retry. Error handling becomes easy due to no partial failures - some ops fails and some succeed
Goal is to simplify the programming model for apps accessing a DB. App is now free to ignore certain potential error scenarios and concurrency issues because DB takes care - these are called "Safety Guarantees". Not all apps need them - adv of weakening trans guarantee or removing is higher performance/availability. Some safety properties can be achieved without.
Relation DBs always had transactions. Nonrelational NoSQL DBs introduced a new data model with replication and partitioning. Transactions were a casualty of this - many systems abandoned transactions or redefined it to mean a much weaker set of safety guarantees.

Meaning of ACID
Safety guarantees provided by transactions are described by acronym ACID - Atomicity, Consistency, Isolation and Durability. Often guarantees provided by "ACID compliant" systems are not clear. Systems that don't meet ACID are called BASE - Basically Available, Soft state and Eventually consistency

a. Atomicity - In multi-threaded, if one thread executes an atomic op, another thread can't see the half finished result (system has two states, one before and one after the atomic op)
But in ACID, atomicity describes what happens if a client wants to make several writes but a fault occurs after some of them are processed - then the transaction can't be completed (committed) due to a fault. Then transaction is aborted and changes discarded/undone. This ensures safe retry because we know changes are discarded and won't be repeated.

b. Consistency - Refers to app specific notion of DB being in a "good state". Idea is that you have certain statements about you data that must always be true - "Invariants" (in acct system, credits and debits must be balanced). If a transaction starts with a DB that is valid according to these invariants, and the transaction writes preserve this validity, then the invariants are always satisfied. This depends on apps notion of invariants and is apps responsibility, not the DB (app decides which data to write, DB only stores it). App may rely on atomicity and isolation properties of the DB in order to achieve consistency.

c. Isolation - DB access from multiple clients is not a problem if they access diff parts of DB. If they access the same parts, then it leads to a race condition. Isolation means that transactions executing concurrently are isolated from each other - can't step on each other. It is also called "Serializability" - each transaction can pretend it's the only transaction running on the entire Db. The DB ensures that when the concurrent transactions have finished executing, the result is the same as if they had run serially "one after another". Serializable Isolation has a high performance impact.

d. Durability - Promise that once a transaction has committed successfully, data written will not be forgotten even if HW fails or DB crashes. In single-node DB, it means data is stored in SSD/hard drive. It usually also involves a write-ahead log or similar which allows recovery in case data structures on disk are corrupted. In replicated DB, it means data is copied to some number of nodes. In order to provide durability guarantee, DB must wait till these writes or replications are complete before reporting transaction as successfully committed. Perfect durability doesn't exist.

Single Object and Multi-Object Operations
Atomicity - DB saves you from worrying about partial failure by guaranteeing all-or-nothing semantics
Isolation - Concurrent transaction should see all updates from other conc transaction or none of the transaction, but not a subset
Multi-object transaction are needed if several pieces of data need to be kept in sync. Example - list of unread emails and number of unread emails.
Need for atomicity - Listing could show unread email but count is 0, because new email is added to list (first write is complete) but counter is not yet incremented (we are seeing state before second write is complete). We should not see half way point - both or none
Need for isolation - If error occurs somewhere in transaction the list and count could be out of sync. If update to counter fails, then transaction is aborted and inserted new email is rolled back from the list
To determine which operations are in same transaction relation DB uses TCP connection to client - on any connection everything between BEGIN TRANSACTION and COMMIT is considered same transaction. Many nonrelational DBs don't have such a semantics for multi-put (put for some documents succeed and don't for others)

Single-object writes
If there is a 20KB json write to DB - if nw connection is interrupted after 10KB have been sent, does DB store the first 10KB; if power fails while DB is in middle of writing to disk, do old and new values end up mixed; if a client reads while another is writing, does it see some partially updated value? 
Storage engines almost always provide atomicity and isolation at level of one object. Atomicity can be done via log for crash recovery and isolation via locking mechanism. Some DBs have more ops like increment (removes need for read-modify-write) and compare-and set (write value allowed only if it's not conc changed by another client)
This prevents lost updates when several client try updating same obj conc. Light-weight transactions.

Need for Multi-object transactions
Many distributed DB don't implement multi-object trans because they are difficult to implement across partitions and compromises high availability and performance. Not all scenarios can be covered by just single-object operations coupled with a key-value data model. Scenarios where multi-object transactions are necessary:
a. In relational data model, a row in one table has a foreign key reference to a row in another table (graph data model has vertex between nodes). MO trans ensures that references remain valid (side - while inserting MOs foreign key needs to be accurate for data to make sense)
b. Document DBs lacking join functionality also encourage denormalization. When denormalization info needs to be updated, several docs need to be updated at one go. MO trans very useful here to prevent data from getting out of sync
c. DBs with secondary indexes (almost everything except pure KV stores), index needs updating when object changes. Indexes are different objects from POV of trans - ex, without transaction isolation document appears in one index but is not updated and shown in another index
Can manage without transactions but appl code becomes too complex w/o atomicity, and conc issues occur w/o isolation

Handling errors and aborts
Key feature of transaction is that it can be safely aborted and retried if an error occurs. If DB is in danger of violating atomicity, isolation or durability guarantees, it prefers to abort and rollback entire transaction rather than be half-finished. In some cases, like datastores with leaderless replication, work on a best-effort basis - DB will do what it can and if it runs into an error, there will be no undoing of work already finished. Hence, it's application's responsibility to recover from errors. Some issues with retrying aborted transactions are:
a. If trans actually succeeded but the nw failed while the server was trying to send acknowledgement of success to the client, then the client think trans failed and it performs retry. So trans gets performed twice, unless appl level de-duplication exists
b. If error is due to an overload, retry will make the situation worse. To avoid such feedback cycles, other mechanisms such can be used - limiting number of retries, using exponential backoff, handle overload-related errors differently
c. It's only worth retrying after transient errors (deadlock, isolation guarantee violation, temporary nw interruptions, failover); After permanent errors (constraint violation - trans succeeded fine but data written was wrong) retry is meaningless
d. If trans has other effects outside the DB, those might happen even if trans is aborted. Ex - we don't want to send an email everytime trans is retried. If we want to make sure several systems either commit or abort together, "Two Phase Commit 2PC" can be used
e. If client process fails while retrying, the data being written is lost
