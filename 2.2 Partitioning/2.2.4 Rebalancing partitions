Changes happen in a DB  - query throughput increase leading to increase in num of CPUs, dataset size increases leading to increase in RAM and disks to store it, machine fails and other machines must take over. These changes require load to be moved between nodes in the cluster. This is called "Rebalancing". Irrespective of the partitioning scheme, Rebalancing is expected to meet these needs:
a. After rebalancing, the load (data storage, read/write requests) should be shared fairly between nodes in the cluster
b. During rebalancing, DB should continue to accept reads/writes
c. Optimum amount of data should be moved between node, to make rebalancing fast and minimize load on network and disk I/O

Strategies for rebalancing

Bad approach is hash mod N - Hashes of the key are divided into ranges and assigned to partitions. Using the value hash mod N to assign to a partition is bad because N, the number of nodes, changes. Thus, hash mod N keeps changing for many of the keys and rebalancing constantly is expensive

Fixed number of partitions - Create many more partitions than there are nodes and assign several partitions to each node. If a new node is added to the cluster, it steals a few partitions from existing nodes until partitions are evenly/fairly distributed again. Node removal causes the reverse process.
* Only entire partitions are moved across nodes - the total number of partitions is unchanged and the assignment of keys to partitions is unchanged. Only assignment of partitions to nodes changes. The change of assignment takes time - large amount of data transfer across nw - and the old partition assignment is used for any reads/writes during this reassignment. (Mismatched hw can potentially be accounted for here by assigning more partitions to more powerful nodes.)
In this config, number of partitions is usually fixed when DB is setup and not changed later. Although it possible to split and merge partitions, due to simplicity many fixed-partition DBs don't suppport partition splitting. Number of partitions chosen initially is the max no of nodes you can have - hence it has to be large enough. But due to mgmt overhead per partition, too many shouldn't be chosen. Choosing right no of partitions is difficult if the dataset size is small initially but much larger later on. Size of each partition grows along with total dataset size. If the partition size is too large, rebalancing and recovering from node failures becomes expensive. Too small means overhead is too much. Optimal number is hard to achieve if num of partitions is fixed but dataset size varies.

Dynamic partitioning - For DBs that use key range partitioning, a fixed number of partitions with fixed boundaries would be very inconvenient - could end up with all data in on partition and all other partitions empty. Reconfiguring partition boundaries manually can be tedious. So, key-partitioned DBs such as HBase create partitions dynamically. When a partition becomes too big, it is split evenly in half. If lots of data is deleted and partition size becomes less than a threshold, adjacent partitions can be merged.
Each partition is assigned to a node and a node contains many partitions. When a partition is split, one half can be transferred to a new node to balance the load. HBase does this transfer via HDFS.
Advantage of dynamic partitioning is that number of partitions adapts to data size - if amount of data is small, number of partitions is small and overhead is small but if amount of data is huge,  the size of a partition is limited to a configurable max
Caveat - when the DB is empty it starts with a single partition since we don't know where to draw partition boundaries. While dataset is small and partition is not split, all requests are processed by a single node and other nodes are idle. HBase and MongoDB allow some initial set of partitions to be configured on an empty DB ("Pre-Splitting"). But in case of key-range partitioning, we should already know the partition boundaries. Dynamic partitioning is suitable for hash-partitioned data as well.

Partitioning proportional to nodes - With dynamic partitioning, number of partitions is proportional to the size of dataset, since splitting and merging keeps size of partition between some min and max. With fixed number of partitions, size of each partition is proportional to the size of dataset. In both cases, num of partitions is independent of num of nodes.
Third option is to make num of partitions proportional to num of nodes - i.e. fixed number of partitions per node. The size of partitions grows proportional to dataset size while the num of nodes is unchanged. But when num of nodes increases, the size of partitions decreases. Since large data volume needs large num of nodes to store, this approach also keeps size of partitions stable
When a new node joins the cluster, it randomly chooses fixed number of existing partitions from other nodes, performs split and takes one half while keeping other half in place. Although, indiviual splits may not be even, with a large num of partitions in a node, overall the new node takes on a fair share of the existing workload.
Picking partition boundaries randomly (while splitting probably) requires that hash-partitioning is used - randomly pick boundaries using hash value). This approach is close to "Consistent Hashing".
