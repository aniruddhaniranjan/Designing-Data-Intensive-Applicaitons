The goal of partitioning is to spread the data and the query load evenly across nodes. If every node takes it's fair share then n nodes should theoretically be able to handle n times the data and n times the read and write throughput of a single node. If partitioning is unfair and some partitions have more data/queries, then the partitioning is called "Skewed". A partition with disproportionate load is called a hot spot - data or query (worst case just 1 node gets majority/all of the data/query). Random partitioning ensures fairness but locating the data is inefficient as all nodes need to be queried in parallel (no way to know which node has the data)

Partition by Key Range - Assign a continuous range of keys to a partition. If you know the boundaries between the ranges, then you know the partition which contains a key. If we know the node which is assigned to a partition, then that node can be queries directly.
Ranges of keys are not necessarily evenly spaced as keys are not evenly distributed. Otherwise some ranges will be much larger than others. Partition boundaries need to adapt to the data, in order for the data to be evenly distributed. Partition boundaries may be chosen manually by admin or automatically by DB.
Within each partition, the keys can be sorted which makes range scans easier. The key can also be treated as a concatenated index which makes executing more complex queries, which fetch several *related* records, in one query easier.
Disadvantage of key range partitioning is that certain access patterns can lead to hot spots. If key is a timestamp, partitions correspond to ranges of time - one partition per day. Then all writes in a day go to the same partition. Example is collecting sensor data with timestamp as the key. This problem can be avoided by having the key as concatenation of sensor ID and timestamp. Then writes for the same day will be more evenly spaced across partitions. But, aggregates across multiple sensors will require querying multiple partitions.

Partition by Hash of Key - A good hash function takes skewed data and makes it uniformly distributed. A 32-bit hash function will take an input such as a string, and return a random number between 0 and 2^32 - 1. Even for similar input, the hash value will be evenly distributed within this range. Hash functions need not be cryptographically strong, and in some languages in-built hash function can even return different hash for same value, which makes them unsuitable.
Each partition can be assigned a range of hashes, and every key whose hash falls in that range will be stored in that partition. This leads to fair distribution. The boundaries can be evenly spaced, or they can be chosen pseudorandomly - consistent hashing (sometimes better called as hashed partitioning)
Disadvantage is that we lose the ability to do efficient range queries. Keys which were adjacent before, are now scattered across partitions. In Cassandra, we can use "Compound Primary Key" in which only the first part of the key is hashed and the other parts are used as a concatenated index for sorting the data. Range of values can't be searched within first column which is hashed, but with a specific value for first column, range scans can be performed against other columns. Useful for one-many relationships such as user posts in social media in a time range, if PK for updates is (userId, timestamp) and only the userId is hashed

Skewed Workloads and Relieving Hot Spots - Hashing a key can reduce hot spots but in edge case of all writes being for the same key, we still get all writes to the same partition. Ex. social media posts by celebrity with millions of followers can cause a storm of activity. Can result in large volume of writes to the same key (userID of celebrity). Hashing doesn't help here since hash of two identical IDs is the same.
Most data systems are not able to automatically compensate for such highly skewed workloads, so application must reduce the skew. One approach is to add a random number to beginning or end of the key. This would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different partitions. But, while performing reads we need to track which keys are being split due to high write throughput, and when read queries are performed for all 100 keys, combine the results from these.
